{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I will use the following notebook to demonstrate different steps in preprocessing\n",
    "\n",
    "## These steps will include:\n",
    "\n",
    "### 1) Slice timing correction\n",
    "### 2) Motion correction\n",
    "### 3) Coregistration\n",
    "### 4) Spatial and temporal filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Import new things that we'll need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nipype.interfaces.afni as afni\n",
    "import nipype.interfaces.fsl as fsl\n",
    "import nipype.interfaces.freesurfer as fs\n",
    "from nipype.interfaces.utility import Function\n",
    "import seaborn as sns\n",
    "import nibabel as nb\n",
    "import json\n",
    "import nipype.interfaces.io as nio\n",
    "import nipype.pipeline.engine as pe \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I next want to get a list of all of my functional files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = ['021']\n",
    "base_dir = '/home/hlee053/Documents/mattfeld_2020'\n",
    "work_dir = '/scratch/hlee053'\n",
    "func_dir = os.path.join(base_dir, f'dset/sub-{sid[0]}/func')\n",
    "fmap_dir = os.path.join(base_dir, f'dset/sub-{sid[0]}/fmap')\n",
    "fs_dir = os.path.join(base_dir, 'derivatives', 'freesurfer')\n",
    "\n",
    "# Get a list of my study task json and nifti converted files\n",
    "func_json = sorted(glob(func_dir + '/*.json'))\n",
    "func_files = sorted(glob(func_dir + '/*.nii.gz'))\n",
    "fmap_files = sorted(glob(fmap_dir + '/*func*.nii.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hlee053/Documents/mattfeld_2020/dset/sub-021/fmap/sub-021_acq-func_dir-AP_run2_epi.nii.gz'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fmap_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next I want to build and run function to perform slice timing correction. I'm going to have to extract some important information from the .json files like the multiband slicetiming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I am building a function that eliminates the\n",
    "# mapnode directory structure and assists in saving\n",
    "# all of the outputs into a single directory\n",
    "def get_subs(func_files):\n",
    "    '''Produces Name Substitutions for Each Contrast'''\n",
    "    subs = []\n",
    "    for curr_run in range(len(func_files)):\n",
    "        subs.append(('_tshifter%d' %curr_run, ''))\n",
    "        subs.append(('_volreg%d' %curr_run, ''))\n",
    "    return subs\n",
    "\n",
    "# Here I am building a function that takes in a\n",
    "# text file that includes the number of outliers\n",
    "# at each volume and then finds which volume (e.g., index)\n",
    "# has the minimum number of outliers (e.g., min) \n",
    "# searching over the first 201 volumes\n",
    "# If the index function returns a list because there were\n",
    "# multiple volumes with the same outlier count, pick the first one\n",
    "def best_vol(outlier_count):\n",
    "    best_vol_num = outlier_count.index(min(outlier_count[:200]))\n",
    "    if isinstance(best_vol_num, list):\n",
    "        best_vol_num = best_vol_num[0]\n",
    "    return best_vol_num\n",
    "\n",
    "# Here I am creating a list of lists containing the slice timing for each study run\n",
    "slice_timing_list = []\n",
    "for curr_json in func_json:\n",
    "    curr_json_data = open(curr_json)\n",
    "    curr_func_metadata = json.load(curr_json_data)\n",
    "    slice_timing_list.append(curr_func_metadata['SliceTiming'])\n",
    "\n",
    "# Here I am establishing a nipype work flow that I will eventually execute\n",
    "psb6351_wf = pe.Workflow(name='psb6351_wf')\n",
    "psb6351_wf.base_dir = work_dir + f'/psb6351workdir/sub-{sid[0]}'\n",
    "psb6351_wf.config['execution']['use_relative_paths'] = True\n",
    "\n",
    "# Create a Function node to substitute names of files created during pipeline\n",
    "getsubs = pe.Node(Function(input_names=['func_files'],\n",
    "                           output_names=['subs'],\n",
    "                           function=get_subs),\n",
    "                  name='getsubs')\n",
    "getsubs.inputs.func_files = func_files\n",
    "\n",
    "# Here I am inputing just the first run functional data\n",
    "# I want to use afni's 3dToutcount to find the number of \n",
    "# outliers at each volume.  I will use this information to\n",
    "# later select the earliest volume with the least number of outliers\n",
    "# to serve as the base for the motion correction\n",
    "id_outliers = pe.Node(afni.OutlierCount(),\n",
    "                      name = 'id_outliers')\n",
    "id_outliers.inputs.in_file = func_files[0]\n",
    "id_outliers.inputs.automask = True\n",
    "id_outliers.inputs.out_file = 'outlier_file'\n",
    "\n",
    "#ATM ONLY: Add an unwarping mapnode here using the field maps\n",
    "calc_distor_corr = pe.Node(afni.Qwarp(),\n",
    "                           name = 'calc_distor_corr')\n",
    "calc_distor_corr.inputs.plusminus = True\n",
    "calc_distor_corr.inputs.pblur = [0.05, 0.05]\n",
    "calc_distor_corr.inputs.minpatch = 9\n",
    "calc_distor_corr.inputs.noweight = True\n",
    "calc_distor_corr.inputs.outputtype = 'NIFTI_GZ'\n",
    "calc_distor_corr.inputs.out_file = 'foobar'\n",
    "calc_distor_corr.inputs.in_file = fmap_files[0]\n",
    "calc_distor_corr.inputs.base_file = fmap_files[1]\n",
    "\n",
    "distor_corr = pe.MapNode(afni.NwarpApply(),\n",
    "                         iterfield=['in_file'],\n",
    "                         name = 'distor_corr')\n",
    "distor_corr.inputs.ainterp = 'quintic'\n",
    "calc_distor_corr.inputs.outputtype = 'NIFTI_GZ'\n",
    "distor_corr.inputs.in_file = func_files\n",
    "psb6351_wf.connect(calc_distor_corr, 'source_warp', distor_corr, 'warp')\n",
    "\n",
    "# Create a Function node to identify the best volume based\n",
    "# on the number of outliers at each volume. I'm searching\n",
    "# for the index in the first 201 volumes that has the \n",
    "# minimum number of outliers and will use the min() function\n",
    "# I will use the index function to get the best vol. \n",
    "getbestvol = pe.Node(Function(input_names=['outlier_count'],\n",
    "                              output_names=['best_vol_num'],\n",
    "                              function=best_vol),\n",
    "                     name='getbestvol')\n",
    "psb6351_wf.connect(id_outliers, 'out_file', getbestvol, 'outlier_count')\n",
    "\n",
    "# Extract the earliest volume with the\n",
    "# the fewest outliers of the first run as the reference \n",
    "extractref = pe.Node(fsl.ExtractROI(t_size=1),\n",
    "                     name = \"extractref\")\n",
    "extractref.inputs.in_file = func_files[0]\n",
    "#extractref.inputs.t_min = int(np.ceil(nb.load(study_func_files[0]).shape[3]/2)) #PICKING MIDDLE\n",
    "psb6351_wf.connect(getbestvol, 'best_vol_num', extractref, 't_min')\n",
    "\n",
    "# Below is the command that runs AFNI's 3dvolreg command.\n",
    "# this is the node that performs the motion correction\n",
    "# I'm iterating over the functional files which I am passing\n",
    "# functional data from the slice timing correction node before\n",
    "# I'm using the earliest volume with the least number of outliers\n",
    "# during the first run as the base file to register to.\n",
    "volreg = pe.MapNode(afni.Volreg(),\n",
    "                    iterfield=['in_file'],\n",
    "                    name = 'volreg')\n",
    "volreg.inputs.outputtype = 'NIFTI_GZ'\n",
    "volreg.inputs.zpad = 4\n",
    "volreg.inputs.in_file = func_files\n",
    "psb6351_wf.connect(extractref, 'roi_file', volreg, 'basefile')\n",
    "\n",
    "# Below is the command that runs AFNI's 3dTshift command\n",
    "# this is the node that performs the slice timing correction\n",
    "# I input the study func files as a list and the slice timing \n",
    "# as a list of lists. I'm using a MapNode to iterate over the two.\n",
    "# this should allow me to parallelize this on the HPC\n",
    "tshifter = pe.MapNode(afni.TShift(),\n",
    "                      iterfield=['in_file','slice_timing'],\n",
    "                      name = 'tshifter')\n",
    "tshifter.inputs.tr = '1.76'\n",
    "tshifter.inputs.slice_timing = slice_timing_list\n",
    "tshifter.inputs.outputtype = 'NIFTI_GZ'\n",
    "psb6351_wf.connect(volreg, 'out_file', tshifter, 'in_file')\n",
    "\n",
    "# Calculate the transformation matrix from EPI space to FreeSurfer space\n",
    "# using the BBRegister command\n",
    "fs_register = pe.Node(fs.BBRegister(init='fsl'),\n",
    "                      name ='fs_register')\n",
    "fs_register.inputs.contrast_type = 't2'\n",
    "fs_register.inputs.out_fsl_file = True\n",
    "fs_register.inputs.subject_id = f'sub-{sid[0]}'\n",
    "fs_register.inputs.subjects_dir = fs_dir\n",
    "psb6351_wf.connect(extractref, 'roi_file', fs_register, 'source_file')\n",
    "\n",
    "# Below is the node that collects all the data and saves\n",
    "# the outputs that I am interested in. Here in this node\n",
    "# I use the substitutions input combined with the earlier\n",
    "# function to get rid of nesting\n",
    "datasink = pe.Node(nio.DataSink(), name=\"datasink\")\n",
    "datasink.inputs.base_directory = os.path.join(base_dir, 'derivatives/preproc')\n",
    "datasink.inputs.container = f'sub-{sid[0]}'\n",
    "psb6351_wf.connect(tshifter, 'out_file', datasink, 'sltime_corr')\n",
    "psb6351_wf.connect(extractref, 'roi_file', datasink, 'study_ref')\n",
    "psb6351_wf.connect(calc_distor_corr, 'source_warp', datasink, 'distortion')\n",
    "psb6351_wf.connect(volreg, 'out_file', datasink, 'motion.@corrfile')\n",
    "psb6351_wf.connect(volreg, 'oned_matrix_save', datasink, 'motion.@matrix')\n",
    "psb6351_wf.connect(volreg, 'oned_file', datasink, 'motion.@par')\n",
    "psb6351_wf.connect(fs_register, 'out_reg_file', datasink, 'register.@reg_file')\n",
    "psb6351_wf.connect(fs_register, 'min_cost_file', datasink, 'register.@reg_cost')\n",
    "psb6351_wf.connect(fs_register, 'out_fsl_file', datasink, 'register.@reg_fsl_file')\n",
    "psb6351_wf.connect(getsubs, 'subs', datasink, 'substitutions')\n",
    "\n",
    "# The following two lines set a work directory outside of my \n",
    "# local git repo and runs the workflow\n",
    "psb6351_wf.run(plugin='SLURM',\n",
    "               plugin_args={'sbatch_args': ('--partition centos7_IB_44C_512G --qos pq_psb6351 --account acc_psb6351'),\n",
    "                            'overwrite':True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now I will load and plot the motion files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_dir = os.path.join(base_dir, f'derivatives/sub-{sid[0]}/motion')\n",
    "study_motion_files = sorted(glob(motion_dir + '/*study*_tshift.1D'))\n",
    "\n",
    "for curr_mot_file in study_motion_files:\n",
    "    motion_df = pd.read_csv(curr_mot_file, sep=\"  \", header=None)\n",
    "    motion_df.columns = ['roll', 'pitch', 'yaw', 'dS', 'dL', 'dP']\n",
    "\n",
    "\n",
    "    num_vols = range(1, len(motion_df)+1)\n",
    "    fig, axs = plt.subplots(motion_df.shape[1], 1, figsize = (15, 10))\n",
    "    # make a little extra space between the subplots\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    for idx, curr_col in enumerate(motion_df.keys()):\n",
    "        axs[idx].plot(num_vols, motion_df[f'{curr_col}'])\n",
    "        axs[idx].set_xlabel('TRs')\n",
    "        axs[idx].set_ylabel(f'{curr_col}')\n",
    "        axs[idx].grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f7d7e2b888ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy_motcorr_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmotion_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/*.nii.gz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy_motcorr_img_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_motcorr_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mstudy_orig_img_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_func_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#test_motcorr_img_data.shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "study_motcorr_files = sorted(glob(motion_dir + '/*.nii.gz'))\n",
    "study_motcorr_img_data = nb.load(study_motcorr_files[0]).get_fdata()\n",
    "study_orig_img_data = nb.load(study_func_files[0]).get_fdata()\n",
    "\n",
    "#test_motcorr_img_data.shape\n",
    "\n",
    "print(study_motcorr_img_data[50,50,32,50])\n",
    "print(study_orig_img_data[50,50,32,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
